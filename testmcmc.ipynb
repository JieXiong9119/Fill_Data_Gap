{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('targets.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    targets_all = next(reader)\n",
    "    \n",
    "print('Total '+str(len(targets_all))+' targets:')\n",
    "print(targets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = './data/'\n",
    "year = '2020'\n",
    "# targets = targets_all\n",
    "# targets = ['AVG_DEMAND_KW_CALCULATED_12','NSN_LF59_BLDG-CDW-LOOP_CDW-FLOW','NSN_LF59_BLDG-CDW-LOOP_CDW-T-R','NSN_LF59_BLDG-CDW-LOOP_CDW-T-S','NSN_LF59_COOLING-TOWER_FAN-HI','NSN_LF59_COOLING-TOWER_FAN-LO','NSN_LF59_COOLING-TOWER_FAN-STAT','NSN_LF59_CWP-1-VFD_VFD-SIG','NSN_LF59_CWP-1_PMP','NSN_LF59_CWP-1_PMP-STAT','NSN_LF59_BOILER-1_HW-T-E','NSN_LF59_BOILER-1_HW-T-L','NSN_LF59_PWSHP-2_CLG-COIL-T','NSN_LF59_PWSHP-2_OA-FLOW','NSN_LF59_PWSHP-2_SA-T','NSN_LF59_PWSHP-2_SA-T-STPT','NSN_LF59_WSHP-01_DA-T','NSN_LF59_WSHP-01_LW-T','NSN_LF59_WSHP-01_ZN-STPT-CL-EFF','NSN_LF59_WSHP-01_ZN-STPT-HT-EFF','NSN_LF59_WSHP-01_ZN-T','NSN_LF59_WSHP-12_DA-T','NSN_LF59_WSHP-12_LW-T','NSN_LF59_WSHP-12_ZN-STPT-CL-EFF','NSN_LF59_WSHP-12_ZN-STPT-HT-EFF','NSN_LF59_WSHP-12_ZN-T','NSN_LF59_BuildingKWhdSum','NSN_LF59_BuildingKWHdSumAnnualized']\n",
    "# targets = [targets_all[0]]\n",
    "targets = ['NSN_LF59_COOLING-TOWER_FAN-LO']\n",
    "for target in targets:\n",
    "    print('Target: '+target)\n",
    "\n",
    "datafilename = 'train_data_'+year+'.csv'\n",
    "data = pd.read_csv(path_to_data+datafilename)\n",
    "\n",
    "X = np.vstack((data.index.to_numpy(), data['HOD'].to_numpy(), data['Environment:Site Outdoor Air Drybulb Temperature [C](TimeStep)'].to_numpy())).T\n",
    "X[:,0] = X[:,0] / 35040.0\n",
    "X[:,1] = X[:,1] / 24.0\n",
    "X[:,2] = (X[:,2]+20.0) / 60.0\n",
    "\n",
    "x_train = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floatX = theano.config.floatX\n",
    "RANDOM_SEED = 12356\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "def construct_nn(X_train, Y_train):\n",
    "    n_hidden = 5\n",
    "\n",
    "    # Initialize random weights between each layer\n",
    "    init_1 = rng.standard_normal(size=(X_train.shape[1], n_hidden)).astype(floatX)\n",
    "    init_2 = rng.standard_normal(size=(n_hidden, n_hidden)).astype(floatX)\n",
    "    init_out = rng.standard_normal(size=n_hidden).astype(floatX)\n",
    "\n",
    "    coords = {\n",
    "        \"hidden_layer_1\": np.arange(n_hidden),\n",
    "        \"hidden_layer_2\": np.arange(n_hidden),\n",
    "        \"train_cols\": np.arange(X_train.shape[1]),\n",
    "        \"obs_id\": np.arange(X_train.shape[0]),\n",
    "    }\n",
    "    with pm.Model(coords=coords) as neural_network:\n",
    "        # Trick: Turn inputs and outputs into shared variables using the data container pm.Data\n",
    "        # It's still the same thing, but we can later change the values of the shared variable\n",
    "        # (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "        # Kind-of like a pointer we can redirect.\n",
    "        # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "        # ann_input = pm.Data(\"ann_input\", X_train)\n",
    "        # ann_output = pm.Data(\"ann_output\", Y_train)\n",
    "\n",
    "        # Weights from input to hidden layer\n",
    "        weights_in_1 = pm.Normal(\n",
    "            \"w_in_1\", 0, sigma=1, testval=init_1, dims=(\"train_cols\", \"hidden_layer_1\")\n",
    "        )\n",
    "\n",
    "        # Weights from 1st to 2nd layer\n",
    "        weights_1_2 = pm.Normal(\n",
    "            \"w_1_2\", 0, sigma=1, testval=init_2, dims=(\"hidden_layer_1\", \"hidden_layer_2\")\n",
    "        )\n",
    "\n",
    "        # Weights from hidden layer to output\n",
    "        weights_2_out = pm.Normal(\"w_2_out\", 0, sigma=1, testval=init_out, dims=\"hidden_layer_2\")\n",
    "\n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pm.math.tanh(pm.math.dot(X_train, weights_in_1))\n",
    "        act_2 = pm.math.tanh(pm.math.dot(act_1, weights_1_2))\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(act_2, weights_2_out))\n",
    "\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Bernoulli(\n",
    "            \"out\",\n",
    "            act_out,\n",
    "            observed=Y_train,\n",
    "            total_size=Y_train.shape[0],  # IMPORTANT for minibatches\n",
    "            dims=\"obs_id\",\n",
    "        )\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for target in targets:\n",
    "    print('Working on ' + target + '...')\n",
    "    y_train = data[target].to_numpy()\n",
    "    y_train = y_train[:, None]\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    neural_network = construct_nn(x_train, y_train)\n",
    "    pm.set_tt_rng(42)\n",
    "    with neural_network:\n",
    "        inference = pm.ADVI()\n",
    "        approx = pm.fit(n=30000, method=inference)\n",
    "\n",
    "    #with open('./model/'+target+'.pkl', 'wb') as file:\n",
    "    #    pickle.dump(m, file)\n",
    "\n",
    "    print('Model for ' + target + ' Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d0451c425ea3e2ce0d674915a8b6b0e3c5c00b771c6523ebafb647fe2c9fe3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
